import numpy as np
import tensorflow as tf
import re
import string
from nltk.corpus import stopwords
import nltk
import asyncio
import logging
import joblib
from aiogram import Bot, Dispatcher
from aiogram.filters import Command
from aiogram.types import Message
from tensorflow.keras.preprocessing.text import tokenizer_from_json
from tensorflow.keras.preprocessing.sequence import pad_sequences
from dotenv import load_dotenv
import os

MAX_VOCAB_SIZE = 25000
MAX_SEQUENCE_LENGTH = 175

load_dotenv()

TOKEN = os.getenv("API_TOKEN")

logging.basicConfig(level=logging.INFO)

bot = Bot(token=TOKEN)
dp = Dispatcher()

scaler = joblib.load("scaler.pkl")

model = tf.keras.models.load_model("bilstm_reviewguard.h5")

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

with open("tokenizer.json", "r") as f:
    tokenizer_data = f.read()
tokenizer = tokenizer_from_json(tokenizer_data)

nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

def clean_text(text):
    text = text.lower()  # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text)  # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    text = re.sub(r"\\d+", "", text)  # –£–¥–∞–ª–µ–Ω–∏–µ —á–∏—Å–µ–ª
    words = text.split()
    words = [word for word in words if word not in stop_words]  # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
    return " ".join(words)

def preprocess_text(text):
    sequence = tokenizer.texts_to_sequences([text])
    padded_sequence = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH, padding="post", truncating="post")
    return padded_sequence


# === –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
def extract_features(text):
    review_length = len(text.split())  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
    avg_word_length = np.mean([len(word) for word in text.split()]) if text.split() else 0  # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞
    num_uppercase = sum(1 for c in text if c.isupper())  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤

    features = np.array([[review_length, avg_word_length, num_uppercase]])
    return scaler.transform(features)

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start
@dp.message(Command("start"))
async def start_handler(message: Message):
    await message.answer(        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–µ–π–∫–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤.\n\n"
        "üîç –û—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞, –∏ —è —Å–∫–∞–∂—É, –Ω–∞—Å—Ç–æ—è—â–∏–π –æ–Ω –∏–ª–∏ –ø–æ–¥–¥–µ–ª—å–Ω—ã–π.\n\n"
        "üìå –ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –æ—Ç–∑—ã–≤ –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏, –∏ —è —Å—Ä–∞–∑—É –µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É—é!"
    )

@dp.message(Command("help"))
async def send_help(message: Message):
    help_text = (
        "üÜò **–ö–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –±–æ—Ç–æ–º?**\n\n"
        "üîπ –ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞, –∏ —è –æ–ø—Ä–µ–¥–µ–ª—é, –Ω–∞—Å—Ç–æ—è—â–∏–π –æ–Ω –∏–ª–∏ —Ñ–µ–π–∫–æ–≤—ã–π.\n"
        "üîπ –ï—Å–ª–∏ –±–æ—Ç –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –∏–ª–∏ –≤–æ–∑–Ω–∏–∫–ª–∏ –æ—à–∏–±–∫–∏, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –µ–≥–æ.\n\n"
        "–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã, –Ω–∞–ø–∏—à–∏—Ç–µ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫—É. üòâ"
    )
    await message.answer(help_text, parse_mode="Markdown")

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
@dp.message()
async def classify_review(message: Message):
    text = message.text

    text_fresh = clean_text(text)
    text_input = preprocess_text(text_fresh)
    features_input = extract_features(text_fresh)

    prediction = model.predict([text_input, features_input])[0][0]

    sentiment = "‚ùå –§–µ–π–∫–æ–≤—ã–π –æ—Ç–∑—ã–≤" if prediction < 0.5 else "‚úÖ –ù–∞—Å—Ç–æ—è—â–∏–π –æ—Ç–∑—ã–≤"

    await message.answer(sentiment)

async def main():
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
